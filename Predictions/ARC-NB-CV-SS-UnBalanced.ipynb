{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"ARC-NB-CV-SS-UnBalanced.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"_S2xzhJkuTEi","colab_type":"code","colab":{},"outputId":"b6512f07-fc5f-4414-a8d4-49d9a0269bc7"},"source":["import os\n","import atexit\n","import sys\n","\n","import pyspark\n","from pyspark.context import SparkContext\n","from pyspark.sql import SQLContext\n","import findspark\n","from sparkhpc import sparkjob\n","\n","#Exit handler to clean up the Spark cluster if the script exits or crashes\n","def exitHandler(sj,sc):\n","    try:\n","        print('Trapped Exit cleaning up Spark Context')\n","        sc.stop()\n","    except:\n","        pass\n","    try:\n","        print('Trapped Exit cleaning up Spark Job')\n","        sj.stop()\n","    except:\n","        pass\n","\n","findspark.init()\n","\n","#Parameters for the Spark cluster\n","nodes=1\n","tasks_per_node=12\n","memory_per_task=1024 #1 gig per process, adjust accordingly\n","# Please estimate walltime carefully to keep unused Spark clusters from sitting \n","# idle so that others may use the resources.\n","walltime=\"24:00\" #1 hour\n","os.environ['SBATCH_PARTITION']='parallel' #Set the appropriate ARC partition\n","\n","sj = sparkjob.sparkjob(\n","     ncores=nodes*tasks_per_node,\n","     cores_per_executor=tasks_per_node,\n","     memory_per_core=memory_per_task,\n","     walltime=walltime\n","    )\n","\n","sj.wait_to_start()\n","sc = sj.start_spark()\n","\n","#Register the exit handler                                                                                                     \n","atexit.register(exitHandler,sj,sc)\n","\n","#You need this line if you want to use SparkSQL\n","sqlCtx=SQLContext(sc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:sparkhpc.sparkjob:Submitted batch job 2616529\n","\n","INFO:sparkhpc.sparkjob:Submitted cluster 0\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_QYKcBXIuTEw","colab_type":"code","colab":{},"outputId":"181b498d-0ea6-4e47-97ad-c2ffdabd5c20"},"source":["df = sqlCtx.read.csv('CleanedNews.csv/part-00000-e0c20413-d9a2-4ae3-bc41-a77b460c6a58-c000.csv',inferSchema=True)\n","df = df.withColumnRenamed('_c0','claim').withColumnRenamed('_c1','claimant').withColumnRenamed('_c2','articles').withColumnRenamed('_c3','label')\n","df.printSchema()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["root\n"," |-- claim: string (nullable = true)\n"," |-- claimant: string (nullable = true)\n"," |-- articles: string (nullable = true)\n"," |-- label: integer (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9wARPduuuTE3","colab_type":"code","colab":{}},"source":["from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer,IDF,StandardScaler,VectorAssembler\n","from pyspark.ml import Pipeline\n","\n","tokenizer = Tokenizer(inputCol='articles',outputCol='token_text')\n","stop_remove = StopWordsRemover(inputCol='token_text',outputCol='stop_token')\n","count_vec = CountVectorizer(inputCol='stop_token',outputCol='c_vec')\n","idf = IDF(inputCol='c_vec',outputCol='tf_idf')\n","ss = StandardScaler(inputCol='tf_idf',outputCol='scaled')\n","assembler = VectorAssembler(inputCols=['scaled'],outputCol='features')\n","\n","pipe = Pipeline(stages=[tokenizer,stop_remove,count_vec,idf,ss,assembler])\n","pipelineFit = pipe.fit(df)\n","dataset = pipelineFit.transform(df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOA1cxAguTE9","colab_type":"code","colab":{}},"source":["training,test = dataset.randomSplit(weights = [0.8,0.2],seed = 0 )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmVKLlnvuTFD","colab_type":"code","colab":{}},"source":["from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n","from pyspark.ml.classification import NaiveBayes\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","nb = NaiveBayes()\n","\n","gridSearch = ParamGridBuilder().addGrid(nb.smoothing,[0.0,0.2,0.4,0.6,0.8,1.0]).build()\n","cvEvaluater = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\",predictionCol=\"prediction\")\n","\n","cv = CrossValidator(estimator=nb,estimatorParamMaps=gridSearch,evaluator=cvEvaluater)\n","cvModel = cv.fit(training)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1oJ_4w0uTFJ","colab_type":"code","colab":{},"outputId":"e433485d-1348-4760-e0c9-4eb610e596f1"},"source":["cvModel.avgMetrics"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.5495986718141771,\n"," 0.5627333232587854,\n"," 0.565204943914567,\n"," 0.5670082755380683,\n"," 0.5682727872583998,\n"," 0.5704466300774521]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"JbW82YY9uTFQ","colab_type":"code","colab":{},"outputId":"b2dc9c65-404d-4863-fd42-690ec5a2c427"},"source":["from sklearn.metrics import classification_report\n","prediction = cvModel.transform(test)\n","y_true = prediction.select('label').collect()\n","y_pred = prediction.select('prediction').collect()\n","print (classification_report(y_true,y_pred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.65      0.65      1500\n","           1       0.57      0.56      0.57      1281\n","           2       0.15      0.16      0.15       341\n","\n","    accuracy                           0.56      3122\n","   macro avg       0.45      0.45      0.45      3122\n","weighted avg       0.56      0.56      0.56      3122\n","\n"],"name":"stdout"}]}]}